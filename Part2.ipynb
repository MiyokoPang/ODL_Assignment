{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Model Building, Tuning, and Evaluation\n",
    "\n",
    "This notebook covers Tasks 1, 2, and 3 of Assignment 2. \n",
    "\n",
    "1.  **Task 1 (Model Building):** We will build two baseline Deep Learning models for this binary classification task.\n",
    "2.  **Task 2 (Model Tuning):** We will use `Keras Tuner` to find the optimal hyperparameters for our model.\n",
    "3.  **Task 3 (Model Evaluation):** We will train the final, tuned model and evaluate its performance on the unseen test set using various metrics.\n",
    "\n",
    "This notebook uses the preprocessed, scaled, and balanced data (`train_final.csv`) and the held-out test set (`test_final.csv`) created in the `01_EDA.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 1. Setup and Load Data\n",
    "\n",
    "# General libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, InputLayer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Keras Tuner\n",
    "import keras_tuner\n",
    "\n",
    "# Scikit-learn for metrics and splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    roc_curve, \n",
    "    auc,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2-load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed data from the EDA notebook\n",
    "base_dir = Path.cwd()\n",
    "train_df = pd.read_csv(base_dir / 'train_final.csv')\n",
    "test_df = pd.read_csv(base_dir / 'test_final.csv')\n",
    "\n",
    "# Display the first few rows to confirm\n",
    "print(\"Training Data (Balanced with SMOTE):\")\n",
    "display(train_df.head())\n",
    "\n",
    "print(\"\\nTest Data (Held-out):\")\n",
    "display(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2-prepare-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data for Modeling\n",
    "\n",
    "# The test_df is our final, unseen hold-out set.\n",
    "X_test_final = test_df.drop('Exited', axis=1)\n",
    "y_test_final = test_df['Exited']\n",
    "\n",
    "# The train_df is already balanced via SMOTE.\n",
    "# We will split this *balanced* set into a training and validation set for the tuning process.\n",
    "# This ensures the tuner validates against data it hasn't seen, while the final test set remains completely untouched.\n",
    "X = train_df.drop('Exited', axis=1)\n",
    "y = train_df['Exited']\n",
    "\n",
    "X_train_tune, X_val_tune, y_train_tune, y_val_tune = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Get the input shape for the neural network\n",
    "input_shape = [X_train_tune.shape[1]]\n",
    "\n",
    "print(f\"Tuning Train Set Shape: {X_train_tune.shape}\")\n",
    "print(f\"Tuning Validation Set Shape: {X_val_tune.shape}\")\n",
    "print(f\"Final Test Set Shape: {X_test_final.shape}\")\n",
    "print(f\"Input Features: {input_shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2 - Task 1: Model Building\n",
    "\n",
    "As per the assignment requirements, we will build two different Deep Learning (Neural Network) models as baselines. Since this is a tabular binary classification problem, Multi-Layer Perceptrons (MLPs) are a suitable choice.\n",
    "\n",
    "1.  **Model 1: Simple MLP:** A basic model with two hidden layers.\n",
    "2.  **Model 2: Deeper MLP:** A slightly deeper model with more neurons and dropout for regularization.\n",
    "\n",
    "We will use `binary_crossentropy` as the loss function, `Adam` as the optimizer, and track `accuracy` and `AUC` as metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2-model-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model 1: Simple MLP\n",
    "simple_mlp = Sequential([\n",
    "    InputLayer(shape=input_shape),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid') # Sigmoid for binary classification\n",
    "])\n",
    "\n",
    "# Compile Model 1\n",
    "simple_mlp.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "\n",
    "simple_mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2-model-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model 2: Deeper MLP\n",
    "deeper_mlp = Sequential([\n",
    "    InputLayer(shape=input_shape),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile Model 2\n",
    "deeper_mlp.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "\n",
    "deeper_mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2-callbacks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Callbacks\n",
    "# We use EarlyStopping to prevent overfitting. It monitors the validation loss \n",
    "# and stops training if it doesn't improve for a set number of epochs (patience).\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=10, \n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2-train-baselines",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Baseline Models\n",
    "print(\"--- Training Simple MLP ---\")\n",
    "history_simple = simple_mlp.fit(\n",
    "    X_train_tune,\n",
    "    y_train_tune,\n",
    "    epochs=100, \n",
    "    batch_size=32,\n",
    "    validation_data=(X_val_tune, y_val_tune),\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n--- Training Deeper MLP ---\")\n",
    "history_deeper = deeper_mlp.fit(\n",
    "    X_train_tune,\n",
    "    y_train_tune,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val_tune, y_val_tune),\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2-plot-history",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to plot training history\n",
    "def plot_history(history, model_name):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot Loss\n",
    "    ax1.plot(history.history['loss'], label='Train Loss')\n",
    "    ax1.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    ax1.set_title(f'{model_name} - Model Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "\n",
    "    # Plot Accuracy\n",
    "    ax2.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    ax2.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    ax2.set_title(f'{model_name} - Model Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_history(history_simple, \"Simple MLP\")\n",
    "plot_history(history_deeper, \"Deeper MLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2-eval-baselines",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Model Evaluation (on the tuning validation set)\n",
    "print(\"--- Simple MLP Evaluation ---\")\n",
    "simple_mlp.evaluate(X_val_tune, y_val_tune)\n",
    "\n",
    "print(\"\\n--- Deeper MLP Evaluation ---\")\n",
    "deeper_mlp.evaluate(X_val_tune, y_val_tune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2 - Task 2: Model Tuning (Hyperparameter Optimization)\n",
    "\n",
    "Now we will tune the hyperparameters of our model to find an optimal configuration. We will use **Keras Tuner** with the `RandomSearch` strategy, as it is more efficient than a full Grid Search.\n",
    "\n",
    "The meta/hyperparameters we will tune are:\n",
    "* **Units (Layer 1):** Number of neurons in the first hidden layer.\n",
    "* **Units (Layer 2):** Number of neurons in the second hidden layer.\n",
    "* **Dropout Rate:** The dropout rate to apply after the first layer for regularization.\n",
    "* **Learning Rate:** The step size for the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2-hp-build",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hypermodel building function\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(shape=input_shape))\n",
    "    \n",
    "    # Tune the number of units in the first Dense layer\n",
    "    hp_units_1 = hp.Int('units_1', min_value=32, max_value=128, step=32)\n",
    "    model.add(Dense(units=hp_units_1, activation='relu'))\n",
    "    \n",
    "    # Tune the dropout rate\n",
    "    hp_dropout_1 = hp.Float('dropout_1', min_value=0.1, max_value=0.5, step=0.1)\n",
    "    model.add(Dropout(rate=hp_dropout_1))\n",
    "    \n",
    "    # Tune the number of units in the second Dense layer\n",
    "    hp_units_2 = hp.Int('units_2', min_value=16, max_value=64, step=16)\n",
    "    model.add(Dense(units=hp_units_2, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Tune the learning rate for the optimizer\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp_learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2-run-tuner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Tuner (RandomSearch)\n",
    "# We use 'val_auc' as the objective to maximize, as it's a robust metric for classification.\n",
    "tuner = keras_tuner.RandomSearch(\n",
    "    build_model,\n",
    "    objective=keras_tuner.Objective(\"val_auc\", direction=\"max\"),\n",
    "    max_trials=10,  # Number of different HP combinations to try\n",
    "    executions_per_trial=2, # Number of times to train each combination for stability\n",
    "    directory='keras_tuner_dir',\n",
    "    project_name='churn_optimization',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Run the search\n",
    "# We use our tuning validation set here. The tuner will use this to score the models.\n",
    "tuner.search(\n",
    "    X_train_tune,\n",
    "    y_train_tune,\n",
    "    epochs=50,\n",
    "    validation_data=(X_val_tune, y_val_tune),\n",
    "    callbacks=[early_stopping] # Use early stopping to speed up the search\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2-tuner-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results of the hyperparameter search\n",
    "print(\"--- Hyperparameter Search Results ---\")\n",
    "tuner.results_summary()\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(\"\\n--- Best Hyperparameters Found ---\")\n",
    "print(f\"Units (Layer 1): {best_hps.get('units_1')}\")\n",
    "print(f\"Dropout (Layer 1): {best_hps.get('dropout_1'):.2f}\")\n",
    "print(f\"Units (Layer 2): {best_hps.get('units_2')}\")\n",
    "print(f\"Learning Rate: {best_hps.get('learning_rate')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2-train-final",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and Train the Final Model\n",
    "\n",
    "# Build the model with the best HPs\n",
    "final_model = tuner.hypermodel.build(best_hps)\n",
    "final_model.summary()\n",
    "\n",
    "# Now, we train the final, optimized model on the *entire* balanced training set (X, y)\n",
    "# and validate it against our *unseen* final test set (X_test_final, y_test_final).\n",
    "\n",
    "print(\"\\n--- Training Final Tuned Model ---\")\n",
    "history_final = final_model.fit(\n",
    "    X, # Full balanced training features\n",
    "    y, # Full balanced training labels\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test_final, y_test_final), # Validate on the unseen test set\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2 - Task 3: Model Evaluation & Discussion\n",
    "\n",
    "We will now evaluate the performance of our `final_model` on the unseen test data (`X_test_final`, `y_test_final`).\n",
    "\n",
    "### Evaluation Metrics Description:\n",
    "* **Accuracy:** The proportion of correct predictions (TP+TN) / (Total). It's a good general metric but can be misleading if classes are imbalanced (though we handled this with SMOTE in the training set).\n",
    "* **Precision:** The proportion of true positives (TP) / (TP + FP). It measures how many of the positive predictions were correct. High precision is important when the cost of a False Positive is high.\n",
    "* **Recall (Sensitivity):** The proportion of true positives (TP) / (TP + FN). It measures how many of the actual positive cases were correctly identified. High recall is crucial when the cost of a False Negative is high (e.g., failing to predict a customer will churn).\n",
    "* **F1-Score:** The harmonic mean of Precision and Recall (2 * (Precision * Recall) / (Precision + Recall)). It provides a single score that balances both metrics.\n",
    "* **ROC-AUC:** (Receiver Operating Characteristic - Area Under the Curve). This measures the model's ability to distinguish between the positive and negative classes across all thresholds. An AUC of 1.0 is a perfect classifier, while 0.5 is no better than random chance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2-plot-final-history",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training history of the final model\n",
    "plot_history(history_final, \"Final Tuned Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2-final-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Evaluation on the Test Set\n",
    "print(\"--- Final Model Evaluation on Unseen Test Set ---\")\n",
    "loss, accuracy, auc_score = final_model.evaluate(X_test_final, y_test_final)\n",
    "print(f\"\\nTest Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test AUC: {auc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2-classification-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for classification report\n",
    "y_pred_proba = final_model.predict(X_test_final)\n",
    "y_pred_classes = (y_pred_proba > 0.5).astype(int) # Using 0.5 threshold\n",
    "\n",
    "print(\"\\n--- Classification Report ---\")\n",
    "print(classification_report(y_test_final, y_pred_classes, target_names=['Stayed (0)', 'Exited (1)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2-confusion-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Confusion Matrix\n",
    "cm = confusion_matrix(y_test_final, y_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Predicted Stayed (0)', 'Predicted Exited (1)'],\n",
    "            yticklabels=['Actual Stayed (0)', 'Actual Exited (1)'])\n",
    "plt.title('Confusion Matrix on Test Set')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2-roc-curve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test_final, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critical Analysis & Discussion (Placeholder)\n",
    "\n",
    "*(This section is for you to complete in your report based on the results above.)*\n",
    "\n",
    "**Analysis of Results:**\n",
    "* **Performance:** The final tuned model achieved a test accuracy of approximately **[Insert Accuracy from Cell 21]** and an AUC of **[Insert AUC from Cell 21]**. \n",
    "* **Baseline Comparison:** This performance compares to the baseline 'Simple MLP' (Accuracy: [~X%], AUC: [~Y%]) and 'Deeper MLP' (Accuracy: [~X%], AUC: [~Y%]) as follows... [Discuss if the tuning provided a significant improvement].\n",
    "* **Classification Report:** The classification report shows... \n",
    "    * **Precision (Exited):** [Insert Precision for 'Exited (1)']. This means that when the model predicts a customer will churn, it is correct [X]% of the time.\n",
    "    * **Recall (Exited):** [Insert Recall for 'Exited (1)']. This is a key metric for this problem. It means the model successfully identified [Y]% of all customers who *actually* churned. \n",
    "* **Confusion Matrix:** The confusion matrix visually confirms this. We see [X] True Positives (correctly predicted churn) and [Y] True Negatives (correctly predicted stay). More importantly, we have [X] False Negatives (customers we *failed* to identify as churners) and [Y] False Positives (customers we incorrectly flagged as churners). \n",
    "* **Model Strengths/Weaknesses:** [Discuss if the model is better at finding churners (high recall) or being certain about its churn predictions (high precision). Discuss the trade-off. The SMOTE on the training data was intended to improve recall, did it work?]\n",
    "* **ROC/AUC:** The AUC score of [Insert AUC] indicates that the model has a [Good/Excellent/Fair] ability to distinguish between a customer who will churn and one who will stay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "*End of Assignment 2, Tasks 1-3. Task 4 (Conclusion) is a written task for the final report.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}